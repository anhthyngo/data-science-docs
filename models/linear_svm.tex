\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{pgfplots}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fullpage}
\usepackage{tikz}
\usepackage{tikz-qtree,tikz-qtree-compat}
\usetikzlibrary{calc}
\setlength{\parskip}{1em}
\begin{document}


\begin{center}
    \Large \textbf{Support Vector Machines (SVM)} \\
    \small Anhthy Ngo
\end{center}

\section{The Support Vector Machine}



For a linear support vector machine (SVM), we use the hypothesis space of affine functions

$$\mathcal{F} = \{f(x) = w^{T}x + b \mid w \in \mathbb{R}^{d},b \in \mathbb{R}\}$$\\
and evaluate with respect to the $\textbf{SVM loss function}$, also known as the $\textbf{hinge loss}$. The hinge loss is a margin loss defined as $\ell(m) = (1-m)_{+}$ where $m = yf(x)$ is the margin for the prediction function $f$ on the example $(x,y)$ and $(x)_{+}$ = $x1(x \geq 0)$ denotes the "positive part" of $x$. The SVM traditionally uses an $\ell 2$ regularization term, and the objective function is written as:
$$J(w,b) = \frac{1}{2} \lvert \lvert w \rvert \rvert^{2} + \frac{c}{n} \sum_{i=1}^{n} (1 - y_{i}[w^{T}x_{i}+b])_{+}$$\\

\subsection{Decision Rule}

\begin{center}
  \tikzset{
    leftNode/.style={circle,minimum width=.5ex, fill=none,draw},
    rightNode/.style={circle,minimum width=.5ex, fill=black,thick,draw},
    rightNodeInLine/.style={solid,circle,minimum width=.7ex, fill=black,thick,draw=white},
    leftNodeInLine/.style={solid,circle,minimum width=.7ex, fill=none,thick,draw},
  }
  \begin{tikzpicture}[
        scale=2,
        important line/.style={thick}, dashed line/.style={dashed, thin},
        every node/.style={color=black},
    ]
    \draw[dashed line, yshift=.7cm]
       (.2,.2) coordinate (sls) -- (2.5,2.5) coordinate (sle)
       node[solid,circle,minimum width=2.8ex,fill=none,thick,draw] (name) at (2,2){}
       node[leftNodeInLine] (name) at (2,2){}
       node[solid,circle,minimum width=2.8ex,fill=none,thick,draw] (name) at (1.5,1.5){}
       node[leftNodeInLine] (name) at (1.5,1.5){}
       node [above right] {$w^{T}x + b > 1$};

    \draw[important line]
       (.7,.7) coordinate (lines) -- (3,3) coordinate (linee)
       node [above right] {$w^{T}x + b = 0$};

    \draw[dashed line, xshift=.7cm]
       (.2,.2) coordinate (ils) -- (2.5,2.5) coordinate (ile)
       node[solid,circle,minimum width=2.8ex,fill=none,thick,draw] (name) at (1.8,1.8){}
       node[rightNodeInLine] (name) at (1.8,1.8){}
       node [above right] {$w^{T} x + b < -1$};

    \draw[very thick,<->] ($(sls)+(.2,.2)$) -- ($(ils)+(.2,.2)$)
       node[sloped,above, near end] {Margin = $\frac{2}{\lVert w \rVert}$};

    \foreach \Point in {(.9,2.4), (1.3,2.5), (1.3,2.1), (2,3), (1,2.9)}{
      \draw \Point node[leftNode]{};
    }

    \foreach \Point in {(2.9,1.4), (2.3,.5), (3.3,.1), (2,0.9), (2.5,1)}{
      \draw \Point node[rightNode]{};
    }
  \end{tikzpicture}
\end{center}
The class $\textit{y}$ is determined as follows:
$$y_{i} = \begin{cases}
-1 & \text{if  } w^{T}x_{i} + b \leq -1 \\
1 & \text{if  } w^{T}x_{i} + b \geq 1 \\
\end{cases}
$$
which can be more concisely written as $y_{i}(w^{T}x_{i} + b) \geq 1$.

\subsection{Goal of SVM}
\begin{enumerate}
  \item Maximize the distance between the two decision boundaries. Mathematically, this means that we want to maximize the distance between the hyperplane defined by $w^{T}x + b = -1$ and the hyperplane defined by $w^{T}x + b = 1$. The distance between hyperplanes is equal to $\frac{2}{\lvert \lvert w \rvert \rvert}$. This means that we want to solve $\underset{x}{\text{max}} \frac{2}{\lVert w \rVert}$, which is equivalent to $\underset{x}{\text{min}} \frac{\lVert w \rVert}{2}^{2}$.

  \item \textbf{Hard-Margin SVM:} The SVM should also correctly classify all $x_{i}$, which mathematically means
  $$y_{i}(w^{T}x_{i}+b) \geq 1, \forall i \in \{1,\dots,N\}$$
  This leads us to the following quadratic optimization problem:
  \begin{equation}
    \begin{aligned}
    \min_{w,b} \quad & \frac{\lVert w \rVert}{2}^{2}\\
    \textrm{s.t.} \quad & y_{i}(w^{T}x_{i}+b) \geq 1 \text{  },\forall i \in \{1,\dots,N\}
    \end{aligned}
    \end{equation}
We call this the \textbf{hard-margin SVM}, as this quadratic optimization problems only admits a solution iff the data is linearly separable. In other words, this variation of SVM \underline{does not} allow for misclassifications.

  \item \textbf{Soft-Margin SVM:} One can relax the constraints by adding $\textbf{"slack variables"}$ $\xi_{i}$. Note that each sample of the training set has its own slack variable. This gives us the following quadratic optimization problem:

  \begin{equation}
    \begin{aligned}
    \min_{w,b} \quad & \frac{1}{2} \lVert w \rVert^{2} + \frac{c}{n} \sum_{i=1}^{n} (1-y_{i}[w_{T}x_{i}+b])_{+}
    \end{aligned}
    \end{equation}
The above equation is an unconstrained optimization problem (which is nice), but it's not differentiable. We can formulate an equivalent problem with a differentiable objective, but we will have to add new constraints to make this a constrained optimization problem. We get the following equivalent equation below:
  \begin{equation}
    \begin{aligned}
    \min_{w,b} \quad & \frac{\lVert w \rVert}{2}^{2} + \frac{c}{n} \sum_{i=1}^{n} \xi_{i},\\
    \textrm{s.t.} \quad & \xi_{i} \geq 1 - y_{i}(w^{T}x_{i}+b)  \text{  },\forall i \in \{1,\dots,N\} \\
    & \xi_{i} \geq 0 \text{  },\forall i \in \{1,\dots,N\}
    \end{aligned}
    \end{equation}
\end{enumerate}
\textbf{What is C?} C is a hyperparameter called \textbf{penalty of the error term}. The $C$ parameter tells the SVM how much to avoid misclassifying each training example. For large $C$, the optimization will choose smaller-margin hyperplane even if the hyperplane does a better job of getting all the training points classified correctly. Conversely, for small $C$, the optimization will look for a larger-margin separating hyperplane even if the hyperplane misclassifies more points. For very tiny values of $C$, you should get misclassified examples even if the training data is linearly separable.

\section{Solving Non-Linear Problems with SVM}
One can add even more flexibility by introduction a function $\phi$ that maps the original feature space to a higher dimensional feature space. This allows to solve non-linear problems with linear separators. The quadratic optimization problem becomes:
\begin{equation}
    \begin{aligned}
    \min_{w,b} \quad & \frac{\lVert w \rVert}{2}^{2} + \frac{c}{n} \sum_{i=1}^{n} \xi_{i},\\
    \textrm{s.t.} \quad & \xi_{i} \geq 1 - y_{i}(w^{T}\phi(x_{i})+b)  \text{  },\forall i \in \{1,\dots,N\} \\
    & \xi_{i} \geq 0 \text{  },\forall i \in \{1,\dots,N\}
    \end{aligned}
    \end{equation}

\section{Optimizing SVM Dual Problem}
The quadratic optimization problem can be transformed to another optimization problem called the Lagrangian dual problem (the previous problem is called the \textbf{primal}). The first step is to compute the Langrangian.
$$L(w,b,\xi,\alpha,\lambda) = \frac{1}{2} \lVert w \rVert^{2} + \frac{c}{n} \sum_{i=1}^{n} \xi_{i} + \sum_{i=1}^{n} \alpha (1-y_{i}[w^{T}x_{i}+b]-\xi_{i}) + \sum_{i+1}^{n} \lambda_{i} (-\xi_{i})$$
From our original study of Lagrangian duality, we know that original problem can now be expressed as:
$$\underset{w,b,\xi}{\text{inf}} \underset{\alpha,\lambda \succcurlyeq 0}{\text{sup}} L(w,b,\xi,\alpha,\lambda)$$\\
Since our constraints are affine and this is a convex problem, by Slater's condition we have strong duality so long as the problem is feasible (i.e., so long as there is at least one point in the feasible set). The constraints are satisfied by $w=0$ and $\xi_{i} = 1$ for $i = 1,\dots,n$, so we have \textbf{strong duality}. Thus we get the same result if we solve the following dual problem:
$$\underset{\alpha,\lambda \succcurlyeq 0}{\text{sup}} \underset{w,b,\xi}{\text{inf}} L(w,b,\xi,\alpha,\lambda)$$

The Lagrange dual is the inf over primal variables of the Lagrangian:

\begin{equation}
\begin{split}
g(\alpha, \lambda) & = \underset{w,b,\xi}{\text{inf}} L(w,b,\xi,\alpha,\lambda) \\
 & = \underset{w,b,\xi}{\text{inf}} [\frac{1}{2}w^{T}w + \sum_{i=1}^{n} \xi_{i}(\frac{c}{n} - \alpha_{i} - \lambda_{i}) + \sum_{i=1}^{n} \alpha_{i} (1 - y_{i}[w^{T}x_{i} + b])]
\end{split}
\end{equation}

By taking the partial derivatives with respect to each primal variable, we get the following:
$$w = \sum_{i=1}^{n} \alpha_{i}y_{i}x_{i}$$
$$w = \sum_{i=1}^{n} \alpha_{i}y_{i} = 0$$
$$w = \alpha_{i} + \lambda_{i} = \frac{c}{n}$$

Now, by substituting these conditions back into $L$, we get:
$$g(\alpha, \lambda) = \begin{cases}
\sum_{i=1}^{n} \alpha_{i} - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i}\alpha_{j}y_{i}y_{j}x_{j}^{T}x_{i} &  \sum_{i=1}^{n} \alpha_{i}y{i} = 0 \\
& \alpha_{i} + \lambda_{i} = \frac{c}{n}, \text{  all i}\\
-\infty & \text{otherwise.}
\end{cases}
$$

And we get that the dual problem is $\underset{\alpha,\lambda \succcurlyeq 0}{\text{sup}}g(\alpha,\lambda)$:

\begin{equation}
    \begin{aligned}
    \underset{\alpha,\lambda}{\text{sup}} \quad & \alpha_{i} - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i}\alpha_{j}y_{i}y_{j}x_{j}^{T}x_{i},\\
    \textrm{s.t.} \quad & \sum_{i=1}^{n} \alpha_{i}y_{i} = 0  \\
    & \alpha_{i} + \lambda_{i} = \frac{c}{n}, \text{  all i}
    \end{aligned}
    \end{equation}
But wait.. we can actually eliminate the $\lambda$ variables, replacing the last constraint by $0 \leq \alpha_{i} \leq \frac{c}{n}$ to get our final dual equation:

\begin{equation}
    \begin{aligned}
    \underset{\alpha}{\text{sup}} \quad & \alpha_{i} - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i}\alpha_{j}y_{i}y_{j}x_{j}^{T}x_{i},\\
    \textrm{s.t.} \quad & \sum_{i=1}^{n} \alpha_{i}y_{i} = 0  \\
    & \alpha_{i} \in [0,\frac{c}{n}]
    \end{aligned}
    \end{equation}
We learn $a_{i}$ using the $(x_{i},y_{i})$ of the training set.
\section{Making Predictions with SVM}
Once $\alpha_{i}$ is learned, one can predict the class of a new sample with the feature vector $x^{\text{test}}$ as follows:

\begin{equation}
\begin{split}
y^{\text{test}} & = \text{sign}(w^{T}\phi(x^{\text{test}}) + b) \\
 & = \text{sign}(\sum_{i=1}^{n} \alpha_{i}y_{i} \phi(x_{i})^{T}\phi(x^{\text{test}}) + b)
\end{split}
\end{equation}
The summation $\sum_{i=1}^{N}$ can seem intimidating, since it means we have to sum over all training examples, but the vast majority of $a_{i}$ are actuallly 0 (i.e., $a_{i}$ is sparse). We conclude that $x_{i}$ is a support vector iff the corresponding $a_{i} > 0$. Know that the fact that most $a_{i} > 0$ is a direct consequence of Karush-Kuhn-Tucker (KTT) dual complementarity conditions.


\section{Kernel Trick}
Soft-Margin SVM can handle non-linearly separable data caused by noise, but what if the non-separability is not caused by noise? Can we still separate using SVM? Yes, the technique "kernel trick" solves this problem. Recall the dual optimization problem (7), in which we only care about the dot product $x_j^{T}x_i$. The function that maps $(x,y)$ to the inner product ($\phi(x)^{T}\phi(y))$ is called a kernel function, often denoted by $k$:
$$k(x,y) = \langle \phi(x), \phi(y) \rangle$$
One can choose a $k$ s.t. the inner product is efficient to compute. We define the kernel trick as:

\begin{equation}
    \begin{aligned}
    \underset{\alpha}{\text{sup}} \quad & \alpha_{i} - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i}\alpha_{j}y_{i}y_{j}k(x_i,x_j),\\
    \textrm{s.t.} \quad & \sum_{i=1}^{n} \alpha_{i}y_{i} = 0  \\
    & \alpha_{i} \in [0,\frac{c}{n}]
    \end{aligned}
    \end{equation}
This small change is a powerful trick and we now have the ability to calculate the result of a dot product performed in another space (i.e., we now have the ability to change the kernel function in order to classify non-linearly separable data). This allows to use potentially high dimensional feature spaces at a low computational cost.
\end{document}

\section{Kernelizing the SVM Primal Problem}


%% sources
%% https://davidrosenberg.github.io/mlcourse/Archive/2016/Notes/svm-notes.pdf
%% https://stats.stackexchange.com/questions/23391/how-does-a-support-vector-machine-svm-work
